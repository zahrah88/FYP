# -*- coding: utf-8 -*-
"""FYPpythoncode.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YfaIt2z4fYV8dKP-_IoCb-IdLmhlam9a
"""

from google.colab import files
import pandas as pd

# Prompt the user to upload the datasets
print("Upload the Student_Performance.csv file.")
uploaded_student_performance = files.upload()

print("Upload the StudentPerformanceFactors.csv file.")
uploaded_student_factors = files.upload()

print("Upload the EDI Dummy data.csv file.")
uploaded_edi_data = files.upload()

# Load the datasets into Pandas DataFrames
student_performance_df = pd.read_csv('Student_Performance.csv')
student_factors_df = pd.read_csv('StudentPerformanceFactors.csv')
edi_dummy_data_df = pd.read_csv('EDI Dummy data.csv')

# Display the first few rows of each dataset
print("Student Performance Dataset:")
print(student_performance_df.head())

print("\nStudent Performance Factors Dataset:")
print(student_factors_df.head())

print("\nEDI Dummy Data:")
print(edi_dummy_data_df.head())

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder
from sklearn.impute import SimpleImputer
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
print("Upload the dataset file.")
from google.colab import files
uploaded = files.upload()

# Replace 'Student_Performance.csv' with the actual filename
student_performance = pd.read_csv('Student_Performance.csv')

# --- Data Cleaning ---

# 1. Handling Missing Values
# Numerical columns: Use mean imputation
num_imputer = SimpleImputer(strategy='mean')
numerical_cols = student_performance.select_dtypes(include=['float64', 'int64']).columns
student_performance[numerical_cols] = num_imputer.fit_transform(student_performance[numerical_cols])

# Categorical columns: Use mode imputation
cat_imputer = SimpleImputer(strategy='most_frequent')
categorical_cols = student_performance.select_dtypes(include=['object']).columns
student_performance[categorical_cols] = cat_imputer.fit_transform(student_performance[categorical_cols])

# 2. Encoding Categorical Variables
# Binary Encoding for columns with 'Yes'/'No'
binary_cols = ['Binary_Column_1', 'Binary_Column_2']  # Replace with actual column names
for col in binary_cols:
    if col in student_performance.columns:
        student_performance[col] = student_performance[col].map({'Yes': 1, 'No': 0})

# One-Hot Encoding for non-ordinal categorical variables
one_hot_cols = ['Parental_Education', 'Socioeconomic_Status']  # Replace with actual column names
if all(col in student_performance.columns for col in one_hot_cols):
    one_hot_encoder = OneHotEncoder(sparse=False, drop='first')
    one_hot_encoded = pd.DataFrame(one_hot_encoder.fit_transform(student_performance[one_hot_cols]),
                                   columns=one_hot_encoder.get_feature_names_out(one_hot_cols))
    student_performance = pd.concat([student_performance.drop(columns=one_hot_cols), one_hot_encoded], axis=1)

# Ordinal Encoding for ordinal categorical variables
ordinal_cols = ['Language_Proficiency_Level']  # Replace with actual column names
if all(col in student_performance.columns for col in ordinal_cols):
    ordinal_mapping = [['Beginner', 'Intermediate', 'Advanced']]  # Replace with appropriate levels
    ordinal_encoder = OrdinalEncoder(categories=ordinal_mapping)
    student_performance[ordinal_cols] = ordinal_encoder.fit_transform(student_performance[ordinal_cols])

# 3. Normalizing Numerical Variables
scaler = StandardScaler()
student_performance[numerical_cols] = scaler.fit_transform(student_performance[numerical_cols])

# --- Exploratory Data Analysis (EDA) ---

# Example: Correlation heatmap (exclude non-numeric columns)
numeric_cols = student_performance.select_dtypes(include=['float64', 'int64']).columns
correlation_matrix = student_performance[numeric_cols].corr()

plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap of Features')
plt.show()

# Example: Distribution of attendance
if 'Attendance' in student_performance.columns:
    sns.histplot(student_performance['Attendance'], kde=True, color='blue')
    plt.title('Distribution of Attendance')
    plt.xlabel('Attendance (%)')
    plt.ylabel('Frequency')
    plt.show()

# Example: Boxplot for performance across socioeconomic categories
if 'Socioeconomic_Status_High' in student_performance.columns and 'Performance' in student_performance.columns:
    sns.boxplot(x='Socioeconomic_Status_High', y='Performance', data=student_performance)
    plt.title('Performance Across Socioeconomic Status')
    plt.xlabel('Socioeconomic Status (High)')
    plt.ylabel('Performance')
    plt.show()

# Save cleaned dataset for further analysis
student_performance.to_csv('Cleaned_Student_Performance.csv', index=False)
print("Cleaned dataset saved as 'Cleaned_Student_Performance.csv'.")

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder
from sklearn.impute import SimpleImputer
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
print("Upload the StudentPerformanceFactors.csv file.")
from google.colab import files
uploaded = files.upload()

# Replace 'StudentPerformanceFactors.csv' with the actual filename
performance_factors = pd.read_csv('StudentPerformanceFactors.csv')

# --- Data Cleaning ---

# 1. Handling Missing Values
# Numerical columns: Use mean imputation
num_imputer = SimpleImputer(strategy='mean')
numerical_cols = performance_factors.select_dtypes(include=['float64', 'int64']).columns
performance_factors[numerical_cols] = num_imputer.fit_transform(performance_factors[numerical_cols])

# Categorical columns: Use mode imputation
cat_imputer = SimpleImputer(strategy='most_frequent')
categorical_cols = performance_factors.select_dtypes(include=['object']).columns
performance_factors[categorical_cols] = cat_imputer.fit_transform(performance_factors[categorical_cols])

# 2. Encoding Categorical Variables
# One-Hot Encoding for non-ordinal categorical variables
one_hot_cols = ['Factor_Category']  # Replace with actual column names
if all(col in performance_factors.columns for col in one_hot_cols):
    one_hot_encoder = OneHotEncoder(sparse=False, drop='first')
    one_hot_encoded = pd.DataFrame(one_hot_encoder.fit_transform(performance_factors[one_hot_cols]),
                                   columns=one_hot_encoder.get_feature_names_out(one_hot_cols))
    performance_factors = pd.concat([performance_factors.drop(columns=one_hot_cols), one_hot_encoded], axis=1)

# Ordinal Encoding for ordinal categorical variables
ordinal_cols = ['Factor_Level']  # Replace with actual column names
ordinal_mapping = [['Low', 'Medium', 'High']]  # Replace with appropriate levels
if all(col in performance_factors.columns for col in ordinal_cols):
    ordinal_encoder = OrdinalEncoder(categories=ordinal_mapping)
    performance_factors[ordinal_cols] = ordinal_encoder.fit_transform(performance_factors[ordinal_cols])

# 3. Normalizing Numerical Variables
scaler = StandardScaler()
performance_factors[numerical_cols] = scaler.fit_transform(performance_factors[numerical_cols])

# --- Exploratory Data Analysis (EDA) ---

# Correlation heatmap (only for numerical columns)
numerical_cols = performance_factors.select_dtypes(include=['float64', 'int64']).columns

# Ensure there are numerical columns before attempting to plot
if not numerical_cols.empty:
    plt.figure(figsize=(10, 8))
    sns.heatmap(performance_factors[numerical_cols].corr(), annot=True, cmap='coolwarm')
    plt.title('Correlation Heatmap for Performance Factors')
    plt.show()
else:
    print("No numerical columns found for correlation heatmap.")

# Example: Barplot for factor levels
if 'Factor_Level' in performance_factors.columns:
    sns.countplot(x='Factor_Level', data=performance_factors)
    plt.title('Count of Factor Levels')
    plt.xlabel('Factor Level')
    plt.ylabel('Frequency')
    plt.show()

# Save cleaned dataset for further analysis
performance_factors.to_csv('Cleaned_StudentPerformanceFactors.csv', index=False)
print("Cleaned dataset saved as 'Cleaned_StudentPerformanceFactors.csv'.")

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder
from sklearn.impute import SimpleImputer
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
print("Upload the EDI Dummy data.csv file.")
from google.colab import files
uploaded = files.upload()

# Replace 'EDI Dummy data.csv' with the actual filename
edi_data = pd.read_csv('EDI Dummy data.csv')

# --- Data Cleaning ---

# 1. Handling Missing Values
# Numerical columns: Use mean imputation
num_imputer = SimpleImputer(strategy='mean')
numerical_cols = edi_data.select_dtypes(include=['float64', 'int64']).columns
edi_data[numerical_cols] = num_imputer.fit_transform(edi_data[numerical_cols])

# Categorical columns: Use mode imputation
cat_imputer = SimpleImputer(strategy='most_frequent')
categorical_cols = edi_data.select_dtypes(include=['object']).columns
edi_data[categorical_cols] = cat_imputer.fit_transform(edi_data[categorical_cols])

# 2. Encoding Categorical Variables
# One-Hot Encoding for non-ordinal categorical variables
one_hot_cols = ['Ethnicity', 'Disability_Type']  # Replace with actual column names
if all(col in edi_data.columns for col in one_hot_cols):
    one_hot_encoder = OneHotEncoder(sparse=False, drop='first')
    one_hot_encoded = pd.DataFrame(one_hot_encoder.fit_transform(edi_data[one_hot_cols]),
                                   columns=one_hot_encoder.get_feature_names_out(one_hot_cols))
    edi_data = pd.concat([edi_data.drop(columns=one_hot_cols), one_hot_encoded], axis=1)

# Ordinal Encoding for ordinal categorical variables
ordinal_cols = ['Proficiency_Level']  # Replace with actual column names
ordinal_mapping = [['Beginner', 'Intermediate', 'Advanced']]  # Replace with appropriate levels
if all(col in edi_data.columns for col in ordinal_cols):
    ordinal_encoder = OrdinalEncoder(categories=ordinal_mapping)
    edi_data[ordinal_cols] = ordinal_encoder.fit_transform(edi_data[ordinal_cols])

# 3. Normalizing Numerical Variables
scaler = StandardScaler()
edi_data[numerical_cols] = scaler.fit_transform(edi_data[numerical_cols])

# --- Exploratory Data Analysis (EDA) ---

# Correlation heatmap (only for numerical columns)
numerical_cols = edi_data.select_dtypes(include=['float64', 'int64']).columns

if not numerical_cols.empty:
    plt.figure(figsize=(10, 8))
    sns.heatmap(edi_data[numerical_cols].corr(), annot=True, cmap='coolwarm')
    plt.title('Correlation Heatmap for EDI Data')
    plt.show()
else:
    print("No numerical columns found for correlation heatmap.")

# Example: Distribution of Age
if 'Age' in edi_data.columns:
    sns.histplot(edi_data['Age'], kde=True, color='blue')
    plt.title('Distribution of Age')
    plt.xlabel('Age')
    plt.ylabel('Frequency')
    plt.show()

# Example: Countplot for Disability Type
if 'Disability_Type' in edi_data.columns:
    sns.countplot(x='Disability_Type', data=edi_data)
    plt.title('Count of Disability Types')
    plt.xlabel('Disability Type')
    plt.ylabel('Frequency')
    plt.show()

# Save cleaned dataset for further analysis
edi_data.to_csv('Cleaned_EDI_Dummy_Data.csv', index=False)
print("Cleaned dataset saved as 'Cleaned_EDI_Dummy_Data.csv'.")

# Import necessary libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans

# Step 1: Check the available files in your Colab environment
import os
print("Available files in the current directory:")
print(os.listdir())

# Step 2: Load the cleaned datasets
# Replace these file names with the actual names of your saved datasets
edi_data_path = 'Cleaned_EDI_Dummy_Data.csv'
student_performance_path = 'Cleaned_Student_Performance.csv'
student_factors_path = 'Cleaned_StudentPerformanceFactors.csv'

# Load datasets into DataFrames
edi_data_df = pd.read_csv(edi_data_path)
student_performance_df = pd.read_csv(student_performance_path)
student_factors_df = pd.read_csv(student_factors_path)

# Display a quick overview of the datasets
print("\n--- EDI Data ---")
print(edi_data_df.info())
print(edi_data_df.head())

print("\n--- Student Performance Data ---")
print(student_performance_df.info())
print(student_performance_df.head())

print("\n--- Student Factors Data ---")
print(student_factors_df.info())
print(student_factors_df.head())

# --- Exploratory Data Analysis (EDA) ---

# --- Univariate Analysis ---
# Example: Summary statistics for numerical variables in the student performance dataset
print("\nSummary Statistics for Student Performance Dataset:")
print(student_performance_df.describe())

# Example: Histogram for 'Hours Studied' from the student performance dataset
if 'Hours Studied' in student_performance_df.columns:
    plt.figure(figsize=(10, 6))
    student_performance_df['Hours Studied'].hist(bins=20, color='skyblue', edgecolor='black')
    plt.title('Distribution of Hours Studied')
    plt.xlabel('Hours Studied')
    plt.ylabel('Frequency')
    plt.show()

# --- Categorical Analysis ---
# Frequency counts for categorical columns (e.g., 'Extracurricular Activities')
if 'Extracurricular Activities' in student_performance_df.columns:
    print("\nFrequency Counts for Extracurricular Activities:")
    print(student_performance_df['Extracurricular Activities'].value_counts())

    # Bar chart for 'Extracurricular Activities'
    plt.figure(figsize=(10, 6))
    sns.countplot(data=student_performance_df, x='Extracurricular Activities', palette='Set2')
    plt.title('Extracurricular Activities Participation')
    plt.show()

# --- Multivariate Analysis ---
# Correlation matrix for numerical variables
numeric_columns = ['Hours Studied', 'Previous Scores', 'Performance Index']  # Adjust based on your dataset
if all(col in student_performance_df.columns for col in numeric_columns):
    corr = student_performance_df[numeric_columns].corr()
    plt.figure(figsize=(10, 6))
    sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')
    plt.title('Correlation Matrix')
    plt.show()

# Scatter plot for 'Hours Studied' vs 'Performance Index'
if 'Hours Studied' in student_performance_df.columns and 'Performance Index' in student_performance_df.columns:
    plt.figure(figsize=(10, 6))
    sns.scatterplot(data=student_performance_df, x='Hours Studied', y='Performance Index')
    plt.title('Hours Studied vs Performance Index')
    plt.show()

# --- Optional: K-Means Clustering ---
# Example clustering using selected numerical columns
if all(col in student_performance_df.columns for col in numeric_columns):
    kmeans = KMeans(n_clusters=3, random_state=42)
    student_performance_df['Cluster'] = kmeans.fit_predict(student_performance_df[numeric_columns])
    plt.figure(figsize=(10, 6))
    sns.scatterplot(data=student_performance_df, x='Hours Studied', y='Performance Index', hue='Cluster', palette='viridis')
    plt.title('Clustering of Students')
    plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Bar plot for categorical data: Performance Index
sns.countplot(data=student_performance, x="Performance Index", palette="viridis")
plt.title("Distribution of Performance Index")
plt.show()

# Scatterplot: Hours Studied vs Performance Index
sns.scatterplot(data=student_performance, x="Hours Studied", y="Performance Index", hue="Performance Index", palette="viridis")
plt.title("Hours Studied vs Performance Index")
plt.show()

import statsmodels.api as sm

# Define the independent variables (e.g., Hours Studied, Sleep Hours)
X = student_performance[['Hours Studied', 'Sleep Hours', 'Previous Scores']]
X = sm.add_constant(X)  # Add constant for intercept

# Define the dependent variable (Performance Index)
y = student_performance['Performance Index']

# Run the regression
model = sm.OLS(y, X).fit()
print(model.summary())

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Step 1: Load the cleaned dataset
try:
    # Load the cleaned student performance dataset
    student_performance_cleaned = pd.read_csv("Cleaned_Student_Performance.csv")

    # Check the first few rows to confirm
    print("Dataset loaded successfully!")
    print(student_performance_cleaned.head())
except FileNotFoundError:
    print("Error: File 'Cleaned_Student_Performance.csv' not found. Please check the file path.")
    exit()

# Step 2: Verify column names
print("\nColumns in the dataset:")
print(student_performance_cleaned.columns)

# Ensure column names are clean and standardized
student_performance_cleaned.columns = student_performance_cleaned.columns.str.strip()

# Check for the 'Performance Index' column
if "Performance Index" not in student_performance_cleaned.columns:
    print("\nError: 'Performance Index' column not found in the dataset.")
    exit()

# Bar plot: Distribution of Performance Index
plt.figure(figsize=(12, 8))
sns.histplot(data=student_performance_cleaned, x="Performance Index", kde=False, bins=20, palette="viridis")  # Adjusted bins to 20
plt.title("Distribution of Performance Index", fontsize=14)
plt.xlabel("Performance Index")
plt.ylabel("Count")
plt.xticks(ticks=range(int(student_performance_cleaned["Performance Index"].min()),
                       int(student_performance_cleaned["Performance Index"].max()) + 1, 2), rotation=45)
plt.tight_layout()
plt.show()